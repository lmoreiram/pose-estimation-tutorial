{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np \n",
    "\n",
    "# Import MediaPipe Drawing module\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "# Import MediaPipe Pose module\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "from angle_calculator import calculate_angle\n",
    "from pose_utils import draw_keypoints_and_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Pose Estimation in an image\n",
    "\n",
    "# Define the path to image file \n",
    "image_path = 'curry.jpg'\n",
    "# Read image from specified path\n",
    "\n",
    "\n",
    "# Create pose object \n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=1, static_image_mode=True) as pose:\n",
    "    # Read image from specified path\n",
    "    frame = cv2.imread(image_path)\n",
    "    # Convert the image from BGR (OpenCV default) to RGB as required by MediaPipe.\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # Prevent image from being written to improve performance\n",
    "    image.flags.writeable = False\n",
    "    \n",
    "    # Process the image to detect pose landmarks\n",
    "    results = pose.process(image)\n",
    "   \n",
    "    # Recolor back to BGR\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Render detections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                            mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                            mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                )               \n",
    "        \n",
    "\n",
    "    cv2.imshow('MediaPipe Detection', image)\n",
    "    cv2.waitKey(0)  \n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Keypoints from an image\n",
    "\n",
    "# Define the path to image file \n",
    "image_path = 'curry.jpg'\n",
    "# Read image from specified path\n",
    "frame = cv2.imread(image_path)\n",
    "frame_height, frame_width, _ = frame.shape\n",
    "\n",
    "# Create pose object \n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=1, static_image_mode=True) as pose:\n",
    "    # Convert the image from BGR (OpenCV default) to RGB as required by MediaPipe.\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # Prevent image from being written to improve performance\n",
    "    image.flags.writeable = False\n",
    "    \n",
    "    # Process the image to detect pose landmarks\n",
    "    results = pose.process(image)\n",
    "   \n",
    "    # Recolor back to BGR\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Extract landmarks\n",
    "    try:\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Render detections\n",
    "    draw_keypoints_and_connections(image, landmarks, 'all')  # Use 'right' or 'left' as needed     \n",
    "\n",
    "\n",
    "    right_shoulder = np.rint(np.array([landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x * frame_width,\n",
    "                            landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y * frame_height]))\n",
    "    right_elbow = np.rint(np.array([landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x * frame_width,\n",
    "                            landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y * frame_height]))\n",
    "    right_wrist = np.rint(np.array([landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x * frame_width,\n",
    "                            landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y * frame_height]))   \n",
    "\n",
    "    \n",
    "\n",
    "    a = right_shoulder - right_elbow\n",
    "    b = right_wrist - right_elbow\n",
    "    \n",
    "\n",
    "    hip_angle = round(calculate_angle(a, b, 'elbow'))\n",
    "\n",
    "  \n",
    "    \n",
    "\n",
    "    # Display the angle at the hip position in yellow with a smaller font size\n",
    "    cv2.putText(image, str(hip_angle), \n",
    "                tuple(right_elbow.astype(int)), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1, cv2.LINE_AA)\n",
    "    \n",
    "    \n",
    "    # Show the image\n",
    "    cv2.imshow('Pose', image)\n",
    "    \n",
    "    cv2.waitKey(0)  \n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open video file stream for video capture\n",
    "cap = cv2.VideoCapture('video_sample.avi')\n",
    "## Setup mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5, static_image_mode=False, model_complexity=2) as pose:\n",
    "    # Loops through each frame in the video\n",
    "    while cap.isOpened():\n",
    "        # read frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "      \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "    \n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Render detections\n",
    "        draw_keypoints_and_connections(image, landmarks, 'right')  # Use 'right' or 'left' as needed           \n",
    "        \n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
